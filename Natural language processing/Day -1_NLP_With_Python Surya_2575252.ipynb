{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0e8fed",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2deadec",
   "metadata": {},
   "source": [
    "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging\n",
    "1.Noise Reduction\n",
    "2.Tokenization\n",
    "3.Lowercasing\n",
    "4.Stopword Removal\n",
    "5.Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec2be50",
   "metadata": {},
   "source": [
    "# 2. Describe tokenization in NLP and explain its significance in text processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec1bf7",
   "metadata": {},
   "source": [
    "Tokenization is a key (and mandatory) aspect of working with text data\n",
    "We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n",
    "This process is essential for various natural language processing tasks, as it provides organized input for analysis. Tokenization plays a crucial role in tasks like sentiment analysis, language translation, and information retrieval, enabling better language understanding, feature extraction, and model comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe747848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting\n",
      "\n",
      "Aftr sentence Tokanization:\n",
      " ['The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting']\n",
      "\n",
      "no of sentnces:\n",
      " 1\n",
      "======================================================================\n",
      "\n",
      "Original text:\n",
      " The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting\n",
      "\n",
      "Aftr word Tokanization:\n",
      " ['The', 'regularization', 'parameter', '``', 'C', \"''\", 'in', 'SVM', 'controls', 'the', 'trade-off', 'between', 'maximizing', 'the', 'margin', 'and', 'minimizing', 'training', 'errors', 'A', 'small', 'C', 'focuses', 'on', 'maximizing', 'the', 'margin', ',', 'potentially', 'allowing', 'some', 'misclassifications', '(', 'soft', 'margin', ')', 'and', 'preventing', 'overfitting']\n",
      "\n",
      "no of words:\n",
      " 39\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting\"\"\"\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_sent=sent_tokenize(text)\n",
    "print('Aftr sentence Tokanization:\\n',tokenised_sent)\n",
    "print()\n",
    "print('no of sentnces:\\n',len(tokenised_sent))\n",
    "print('='*70)\n",
    "print()\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print()\n",
    "tokenised_word=word_tokenize(text)\n",
    "print('Aftr word Tokanization:\\n',tokenised_word)\n",
    "print()\n",
    "print('no of words:\\n',len(tokenised_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbcfd31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Prakash\n",
      "[nltk_data]     Nani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c116334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Tokenized words - without stemming:\n",
      "\n",
      "\t ['The', 'regularization', 'parameter', '``', 'C', \"''\", 'in', 'SVM', 'controls', 'the', 'trade-off', 'between', 'maximizing', 'the', 'margin', 'and', 'minimizing', 'training', 'errors', 'A', 'small', 'C', 'focuses', 'on', 'maximizing', 'the', 'margin', ',', 'potentially', 'allowing', 'some', 'misclassifications', '(', 'soft', 'margin', ')', 'and', 'preventing', 'overfitting']\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer stemming are:\n",
      "\t ['the', 'regular', 'paramet', '``', 'c', \"''\", 'in', 'svm', 'control', 'the', 'trade-off', 'between', 'maxim', 'the', 'margin', 'and', 'minim', 'train', 'error', 'a', 'small', 'c', 'focus', 'on', 'maxim', 'the', 'margin', ',', 'potenti', 'allow', 'some', 'misclassif', '(', 'soft', 'margin', ')', 'and', 'prevent', 'overfit']\n",
      "======================================================================\n",
      "lemmarized words:\n",
      " ['The', 'regularization', 'parameter', '``', 'C', \"''\", 'in', 'SVM', 'control', 'the', 'trade-off', 'between', 'maximize', 'the', 'margin', 'and', 'minimize', 'train', 'errors', 'A', 'small', 'C', 'focus', 'on', 'maximize', 'the', 'margin', ',', 'potentially', 'allow', 'some', 'misclassifications', '(', 'soft', 'margin', ')', 'and', 'prevent', 'overfitting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "\n",
    "stemmed_words=[]\n",
    "\n",
    "for w in tokenised_word:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "    \n",
    "\n",
    "print('='*70)\n",
    "print('Tokenized words - without stemming:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer stemming are:\\n\\t',stemmed_words)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemma=WordNetLemmatizer()#Initilize the wordnet lemmatizer\n",
    "\n",
    "#apply lemmatazion to each word\n",
    "lemma_words=[lemma.lemmatize(word,pos='v') for word in tokenised_word ]#apply lemmatazion to each word\n",
    "\n",
    "print('='*70)\n",
    "print('lemmarized words:\\n',lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be707b30",
   "metadata": {},
   "source": [
    "# 4. Explain the concept of stop words and their role in text preprocessing. How do they impact NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834bdf9",
   "metadata": {},
   "source": [
    "stop words usually refers to the most common words in a language. There is no universal list of stop words that is used by all NLP tools in common\n",
    "What are stop words\n",
    "Stopwords are the words: in any language which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as “The Who” or “Take That”\n",
    "When to remove stop words: if we have a task of text classification or sentiment analysis then we should remove stop words as they do not provide any information to our model, i.e keeping out unwanted words out of our corpus, but if we have the task of language translation then stopwords are useful, as they have to be translated along with other word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0166e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words:\t 39\n",
      "======================================================================\n",
      "Tokenized words - with stop words:\n",
      "\n",
      "\t ['The', 'regularization', 'parameter', '``', 'C', \"''\", 'in', 'SVM', 'controls', 'the', 'trade-off', 'between', 'maximizing', 'the', 'margin', 'and', 'minimizing', 'training', 'errors', 'A', 'small', 'C', 'focuses', 'on', 'maximizing', 'the', 'margin', ',', 'potentially', 'allowing', 'some', 'misclassifications', '(', 'soft', 'margin', ')', 'and', 'preventing', 'overfitting']\n",
      "======================================================================\n",
      "Length after the remoal of stopwords:\t 30\n",
      "======================================================================\n",
      "\n",
      "Tokenized words - afer removing the stopwords are:\n",
      "\t ['The', 'regularization', 'parameter', '``', 'C', \"''\", 'SVM', 'controls', 'trade-off', 'maximizing', 'margin', 'minimizing', 'training', 'errors', 'A', 'small', 'C', 'focuses', 'maximizing', 'margin', ',', 'potentially', 'allowing', 'misclassifications', '(', 'soft', 'margin', ')', 'preventing', 'overfitting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens=[]\n",
    "for w in tokenised_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_tokens.append(w)\n",
    "print('Length of words:\\t',len(tokenised_word))\n",
    "print('='*70)\n",
    "print('Tokenized words - with stop words:\\n\\n\\t',tokenised_word)\n",
    "print('='*70)\n",
    "print('Length after the remoal of stopwords:\\t',len(filtered_tokens))\n",
    "print('='*70)\n",
    "print('\\nTokenized words - afer removing the stopwords are:\\n\\t',filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dce4b68",
   "metadata": {},
   "source": [
    "# 5. How does the process of removing punctuation contribute to text preprocessing in NLP? What are its benefits?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb532e5",
   "metadata": {},
   "source": [
    "The second most common text processing technique is removing punctuations from the textual data. The punctuation removal process will help to treat each text equally. For example, the word data and data! are treated equally after the process of removal of punctuations. This process prevents interference with the understanding of language models and enhances tokenization accuracy by establishing clear word boundaries for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e38542e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting\n",
      "======================================================================\n",
      "after removing puntuations:\n",
      "\n",
      "['The', 'regularization', 'parameter', 'C', 'in', 'SVM', 'controls', 'the', 'between', 'maximizing', 'the', 'margin', 'and', 'minimizing', 'training', 'errors', 'A', 'small', 'C', 'focuses', 'on', 'maximizing', 'the', 'margin', 'potentially', 'allowing', 'some', 'misclassifications', 'soft', 'margin', 'and', 'preventing', 'overfitting']\n"
     ]
    }
   ],
   "source": [
    "words=[word for word in tokenised_word if word.isalpha()]\n",
    "\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after removing puntuations:\\n')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d255290",
   "metadata": {},
   "source": [
    "# 6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a common step in NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8d447",
   "metadata": {},
   "source": [
    "Lower casing: Converting a word to lower case (NLP -> nlp). Words like Book and book mean the same but when not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimensions). It is a common practice in NLP to ensure consistency in text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "069e390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " The regularization parameter \"C\" in SVM controls the trade-off between maximizing the margin and minimizing training errors A small C focuses on maximizing the margin, potentially allowing some misclassifications (soft margin) and preventing overfitting\n",
      "======================================================================\n",
      "after lowering the case:\n",
      "\n",
      "['the', 'regularization', 'parameter', '``', 'c', \"''\", 'in', 'svm', 'controls', 'the', 'trade-off', 'between', 'maximizing', 'the', 'margin', 'and', 'minimizing', 'training', 'errors', 'a', 'small', 'c', 'focuses', 'on', 'maximizing', 'the', 'margin', ',', 'potentially', 'allowing', 'some', 'misclassifications', '(', 'soft', 'margin', ')', 'and', 'preventing', 'overfitting']\n"
     ]
    }
   ],
   "source": [
    "lower_words=[word.lower() for word in tokenised_word]\n",
    "print('Original text:\\n',text)\n",
    "print('='*70)\n",
    "\n",
    "print('after lowering the case:\\n')\n",
    "print(lower_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d378e",
   "metadata": {},
   "source": [
    "\n",
    "# 7. Explain the term \"vectorization\" concerning text data. How does techniques like CountVectorizer contribute to text preprocessing in NLP ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865a404",
   "metadata": {},
   "source": [
    "Hence the process of converting text into vector is called vectorization. By using CountVectorizer function we can convert text document to matrix of word count. Matrix which is produced here is sparse matrix. By using CountVectorizer on above document we get 5*15 sparse matrix of type numpy. This approach allows models to interpret text data for tasks like classification or clustering in NLP, ultimately enhancing model performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38ebf909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      " ['allowing' 'and' 'between' 'controls' 'errors' 'focuses' 'in' 'margin'\n",
      " 'maximizing' 'minimizing' 'misclassifications' 'off' 'on' 'overfitting'\n",
      " 'parameter' 'potentially' 'preventing' 'regularization' 'small' 'soft'\n",
      " 'some' 'svm' 'the' 'trade' 'training']\n",
      "**********************************************************************\n",
      "Token counts matrix:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector=CountVectorizer()\n",
    "\n",
    "x=vector.fit_transform(tokenised_word)\n",
    "\n",
    "feature_names=vector.get_feature_names_out()\n",
    "\n",
    "print('Feature names:\\n',feature_names)\n",
    "print('*'*70)\n",
    "print('Token counts matrix:')\n",
    "print(x.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f77438",
   "metadata": {},
   "source": [
    "# 8. Describe the concept of normalization in NLP. Provide examples of normalization techniques used in text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b3f14",
   "metadata": {},
   "source": [
    "Normalization in NLP seeks to convert diverse word forms into a standardized format, improving analysis by minimizing redundancy. Methods like stemming (shortening words to their root form, such as changing \"running\" to \"run\") and lemmatization (reducing words to their base or dictionary form, like transforming \"better\" to \"good\") play a role in standardizing text. Additionally, actions like Punctuation Removal and Stop Word Removal contribute to maintaining consistency in language representation. These practices enhance model comprehension and accuracy, particularly in tasks like sentiment analysis or information retrieval\n",
    "1. Case Normalization\n",
    "2. Punctuation Removal\n",
    "3. Stop Word Removal\n",
    "4. Stemming\n",
    "5. Lemmatization\n",
    "6. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec7272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
